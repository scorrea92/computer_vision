{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 14 17:22:41 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX TIT...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n",
      "|  0%   44C    P0    60W / 250W |      0MiB / 12205MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/secorec/anaconda3/envs/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 250, 250, 3)\n",
      "(791,)\n",
      "(784, 250, 250, 3)\n",
      "(784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70e42e2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers import GaussianNoise as GN\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler as LRS\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 20\n",
    "epochs = 50\n",
    "\n",
    "#### LOAD AND TRANSFORM\n",
    "\n",
    "# ## Download: ONLY ONCE!\n",
    "# !wget https://www.dropbox.com/s/kdhn10jwj99xkv7/data.tgz\n",
    "# !tar xvzf data.tgz\n",
    "# #####\n",
    "\n",
    "\n",
    "# Load \n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Stats\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "## View some images\n",
    "plt.imshow(x_train[2,:,:,: ] )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Transforms\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "## Labels\n",
    "y_train=y_train-1\n",
    "\n",
    "y_test=y_test-1\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 250, 250, 64) 1792        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 250, 250, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 125, 125, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 125, 125, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 125, 125, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 62, 62, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 62, 62, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 62, 62, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 62, 62, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 31, 31, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 31, 31, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 31, 31, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 31, 31, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 31, 31, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 31, 31, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "outer_product (Lambda)          (None, 262144)       0           dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 20)           5242900     outer_product[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 12,878,164\n",
      "Trainable params: 5,242,900\n",
      "Non-trainable params: 7,635,264\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "25/24 [==============================] - 15s 591ms/step - loss: 3.0216 - acc: 0.0588 - val_loss: 2.9115 - val_acc: 0.1212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.91153, saving model to Wehigts.hdf5\n",
      "Epoch 2/50\n",
      "25/24 [==============================] - 10s 415ms/step - loss: 2.8644 - acc: 0.1394 - val_loss: 2.8080 - val_acc: 0.1543\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.91153 to 2.80803, saving model to Wehigts.hdf5\n",
      "Epoch 3/50\n",
      "25/24 [==============================] - 10s 414ms/step - loss: 2.7487 - acc: 0.1976 - val_loss: 2.7567 - val_acc: 0.1811\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.80803 to 2.75669, saving model to Wehigts.hdf5\n",
      "Epoch 4/50\n",
      "25/24 [==============================] - 10s 415ms/step - loss: 2.6594 - acc: 0.2379 - val_loss: 2.6752 - val_acc: 0.2181\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.75669 to 2.67515, saving model to Wehigts.hdf5\n",
      "Epoch 5/50\n",
      "25/24 [==============================] - 10s 414ms/step - loss: 2.5644 - acc: 0.2767 - val_loss: 2.6094 - val_acc: 0.2819\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.67515 to 2.60942, saving model to Wehigts.hdf5\n",
      "Epoch 6/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 2.4933 - acc: 0.3219 - val_loss: 2.5738 - val_acc: 0.1926\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.60942 to 2.57379, saving model to Wehigts.hdf5\n",
      "Epoch 7/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 2.4073 - acc: 0.3766 - val_loss: 2.5290 - val_acc: 0.2564\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.57379 to 2.52899, saving model to Wehigts.hdf5\n",
      "Epoch 8/50\n",
      "25/24 [==============================] - 11s 420ms/step - loss: 2.3523 - acc: 0.3936 - val_loss: 2.4636 - val_acc: 0.3061\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.52899 to 2.46359, saving model to Wehigts.hdf5\n",
      "Epoch 9/50\n",
      "25/24 [==============================] - 10s 419ms/step - loss: 2.2759 - acc: 0.4279 - val_loss: 2.4409 - val_acc: 0.2717\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.46359 to 2.44089, saving model to Wehigts.hdf5\n",
      "Epoch 10/50\n",
      "25/24 [==============================] - 11s 423ms/step - loss: 2.2168 - acc: 0.4549 - val_loss: 2.3716 - val_acc: 0.3431\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.44089 to 2.37157, saving model to Wehigts.hdf5\n",
      "Epoch 11/50\n",
      "25/24 [==============================] - 10s 418ms/step - loss: 2.1517 - acc: 0.4630 - val_loss: 2.3164 - val_acc: 0.3406\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.37157 to 2.31636, saving model to Wehigts.hdf5\n",
      "Epoch 12/50\n",
      "25/24 [==============================] - 10s 418ms/step - loss: 2.0941 - acc: 0.5109 - val_loss: 2.3005 - val_acc: 0.3444\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.31636 to 2.30054, saving model to Wehigts.hdf5\n",
      "Epoch 13/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 2.0337 - acc: 0.5207 - val_loss: 2.2462 - val_acc: 0.3673\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.30054 to 2.24619, saving model to Wehigts.hdf5\n",
      "Epoch 14/50\n",
      "25/24 [==============================] - 11s 421ms/step - loss: 1.9767 - acc: 0.5276 - val_loss: 2.2661 - val_acc: 0.3227\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "25/24 [==============================] - 10s 419ms/step - loss: 1.9388 - acc: 0.5607 - val_loss: 2.1590 - val_acc: 0.4184\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.24619 to 2.15904, saving model to Wehigts.hdf5\n",
      "Epoch 16/50\n",
      "25/24 [==============================] - 11s 422ms/step - loss: 1.8921 - acc: 0.5692 - val_loss: 2.1414 - val_acc: 0.3967\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.15904 to 2.14144, saving model to Wehigts.hdf5\n",
      "Epoch 17/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.8428 - acc: 0.5881 - val_loss: 2.1495 - val_acc: 0.3622\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "25/24 [==============================] - 11s 423ms/step - loss: 1.7921 - acc: 0.6067 - val_loss: 2.1015 - val_acc: 0.4184\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.14144 to 2.10148, saving model to Wehigts.hdf5\n",
      "Epoch 19/50\n",
      "25/24 [==============================] - 11s 421ms/step - loss: 1.7499 - acc: 0.6248 - val_loss: 2.0982 - val_acc: 0.3967\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.10148 to 2.09821, saving model to Wehigts.hdf5\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/24 [==============================] - 10s 417ms/step - loss: 1.7065 - acc: 0.6281 - val_loss: 2.0328 - val_acc: 0.4247\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.09821 to 2.03279, saving model to Wehigts.hdf5\n",
      "Epoch 21/50\n",
      "25/24 [==============================] - 10s 415ms/step - loss: 1.6711 - acc: 0.6384 - val_loss: 1.9924 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.03279 to 1.99237, saving model to Wehigts.hdf5\n",
      "Epoch 22/50\n",
      "25/24 [==============================] - 11s 422ms/step - loss: 1.6373 - acc: 0.6544 - val_loss: 1.9728 - val_acc: 0.4337\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.99237 to 1.97275, saving model to Wehigts.hdf5\n",
      "Epoch 23/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.6080 - acc: 0.6661 - val_loss: 1.9586 - val_acc: 0.4554\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.97275 to 1.95861, saving model to Wehigts.hdf5\n",
      "Epoch 24/50\n",
      "25/24 [==============================] - 11s 420ms/step - loss: 1.5455 - acc: 0.6828 - val_loss: 1.9438 - val_acc: 0.4439\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.95861 to 1.94381, saving model to Wehigts.hdf5\n",
      "Epoch 25/50\n",
      "25/24 [==============================] - 11s 421ms/step - loss: 1.5296 - acc: 0.6753 - val_loss: 1.9213 - val_acc: 0.4579\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.94381 to 1.92129, saving model to Wehigts.hdf5\n",
      "Epoch 26/50\n",
      "25/24 [==============================] - 11s 421ms/step - loss: 1.4853 - acc: 0.7013 - val_loss: 1.8909 - val_acc: 0.4656\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.92129 to 1.89094, saving model to Wehigts.hdf5\n",
      "Epoch 27/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 1.4409 - acc: 0.7192 - val_loss: 1.8932 - val_acc: 0.4439\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.4394 - acc: 0.6936 - val_loss: 1.8432 - val_acc: 0.4770\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.89094 to 1.84319, saving model to Wehigts.hdf5\n",
      "Epoch 29/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.3934 - acc: 0.7152 - val_loss: 1.8231 - val_acc: 0.4898\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.84319 to 1.82313, saving model to Wehigts.hdf5\n",
      "Epoch 30/50\n",
      "25/24 [==============================] - 11s 422ms/step - loss: 1.3579 - acc: 0.7422 - val_loss: 1.8148 - val_acc: 0.4707\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.82313 to 1.81484, saving model to Wehigts.hdf5\n",
      "Epoch 31/50\n",
      "25/24 [==============================] - 10s 419ms/step - loss: 1.3279 - acc: 0.7500 - val_loss: 1.8077 - val_acc: 0.4847\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.81484 to 1.80770, saving model to Wehigts.hdf5\n",
      "Epoch 32/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.3104 - acc: 0.7571 - val_loss: 1.8113 - val_acc: 0.4668\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/50\n",
      "25/24 [==============================] - 10s 419ms/step - loss: 1.2867 - acc: 0.7538 - val_loss: 1.7968 - val_acc: 0.4732\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.80770 to 1.79684, saving model to Wehigts.hdf5\n",
      "Epoch 34/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 1.2577 - acc: 0.7613 - val_loss: 1.7553 - val_acc: 0.5038\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.79684 to 1.75531, saving model to Wehigts.hdf5\n",
      "Epoch 35/50\n",
      "25/24 [==============================] - 11s 421ms/step - loss: 1.2490 - acc: 0.7921 - val_loss: 1.7371 - val_acc: 0.4949\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.75531 to 1.73712, saving model to Wehigts.hdf5\n",
      "Epoch 36/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 1.2230 - acc: 0.7855 - val_loss: 1.7413 - val_acc: 0.4923\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/50\n",
      "25/24 [==============================] - 10s 418ms/step - loss: 1.1719 - acc: 0.7880 - val_loss: 1.7177 - val_acc: 0.5064\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.73712 to 1.71772, saving model to Wehigts.hdf5\n",
      "Epoch 38/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.1830 - acc: 0.7994 - val_loss: 1.7058 - val_acc: 0.4974\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.71772 to 1.70576, saving model to Wehigts.hdf5\n",
      "Epoch 39/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 1.1380 - acc: 0.7813 - val_loss: 1.7020 - val_acc: 0.4949\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.70576 to 1.70196, saving model to Wehigts.hdf5\n",
      "Epoch 40/50\n",
      "25/24 [==============================] - 10s 419ms/step - loss: 1.1220 - acc: 0.7974 - val_loss: 1.6917 - val_acc: 0.5204\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.70196 to 1.69166, saving model to Wehigts.hdf5\n",
      "Epoch 41/50\n",
      "25/24 [==============================] - 11s 423ms/step - loss: 1.1017 - acc: 0.8135 - val_loss: 1.6632 - val_acc: 0.5191\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.69166 to 1.66320, saving model to Wehigts.hdf5\n",
      "Epoch 42/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 1.0780 - acc: 0.8146 - val_loss: 1.6380 - val_acc: 0.5204\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.66320 to 1.63804, saving model to Wehigts.hdf5\n",
      "Epoch 43/50\n",
      "25/24 [==============================] - 11s 422ms/step - loss: 1.0657 - acc: 0.8223 - val_loss: 1.6377 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.63804 to 1.63772, saving model to Wehigts.hdf5\n",
      "Epoch 44/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 1.0330 - acc: 0.8262 - val_loss: 1.6485 - val_acc: 0.5051\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/50\n",
      "25/24 [==============================] - 11s 430ms/step - loss: 1.0172 - acc: 0.8368 - val_loss: 1.6523 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/50\n",
      "25/24 [==============================] - 11s 420ms/step - loss: 1.0016 - acc: 0.8448 - val_loss: 1.6000 - val_acc: 0.5306\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.63772 to 1.59998, saving model to Wehigts.hdf5\n",
      "Epoch 47/50\n",
      "25/24 [==============================] - 10s 417ms/step - loss: 0.9857 - acc: 0.8338 - val_loss: 1.5989 - val_acc: 0.5204\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.59998 to 1.59887, saving model to Wehigts.hdf5\n",
      "Epoch 48/50\n",
      "25/24 [==============================] - 10s 418ms/step - loss: 0.9664 - acc: 0.8355 - val_loss: 1.5717 - val_acc: 0.5332\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.59887 to 1.57168, saving model to Wehigts.hdf5\n",
      "Epoch 49/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 0.9545 - acc: 0.8330 - val_loss: 1.5519 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.57168 to 1.55191, saving model to Wehigts.hdf5\n",
      "Epoch 50/50\n",
      "25/24 [==============================] - 10s 416ms/step - loss: 0.9325 - acc: 0.8690 - val_loss: 1.5619 - val_acc: 0.5357\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load the model\n",
    "model1 = VGG16(weights='imagenet', include_top=False, input_shape=x_train.shape[1:])\n",
    "\n",
    "#############################\n",
    "###      BILINEAR        ####\n",
    "#############################\n",
    "\n",
    "# No entrenar la VGG\n",
    "for layer in model1.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "def outer_product(x):\n",
    "  phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\t\t# Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "  phi_I = tf.reshape(phi_I,[-1,512*512])\t        # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "  phi_I = tf.divide(phi_I,31*31)\t\t\t\t\t\t\t\t  # Divide by feature map size [sizexsize]\n",
    "\n",
    "  y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\t\t# Take signed square root of phi_I\n",
    "  z_l2 = tf.nn.l2_normalize(y_ssqrt, dim=1)\t\t\t\t\t\t\t\t              # Apply l2 normalization\n",
    "  return z_l2\n",
    "\n",
    "\n",
    "\n",
    "conv=model1.get_layer('block4_conv3') # block4_conv3\n",
    "d1=Dropout(0.5)(conv.output)   ## Why??\n",
    "d2=Dropout(0.5)(conv.output)   ## Why??\n",
    "\n",
    "x = Lambda(outer_product, name='outer_product')([d1,d2])\n",
    "predictions=Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#layer_x=Dense(256, activation='relu', name='midle_layer')(x)\n",
    "#predictions=Dense(num_classes, activation='softmax', name='predictions')(layer_x)\n",
    "\n",
    "model = Model(inputs=model1.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# DEFINE A LEARNING RATE SCHEDULER\n",
    "def scheduler(epoch):\n",
    "    if epoch < 25:\n",
    "        return 0.0001\n",
    "    elif epoch < 50:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.000001\n",
    "\n",
    "set_lr = LRS(scheduler)\n",
    "\n",
    "\n",
    "## DATAGEN\n",
    "datagen = ImageDataGenerator(\n",
    "  width_shift_range=0.2,\n",
    "  height_shift_range=0.2,\n",
    "  rotation_range=20,\n",
    "  zoom_range=[1.0,1.2],\n",
    "  horizontal_flip=True)\n",
    "\n",
    "\n",
    "## OPTIM AND COMPILE use Adam Rsmprop\n",
    "opt = SGD(lr=0.0001, decay=1e-6)\n",
    "rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "  \n",
    "model.summary()\n",
    "\n",
    "checkpoint_path = \"Wehigts.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "## TRAINING with DA and LRA\n",
    "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                            steps_per_epoch=len(x_train) / batch_size, \n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            callbacks=[checkpointer],\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 250, 250, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 250, 250, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 125, 125, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 125, 125, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 125, 125, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 62, 62, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 62, 62, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 62, 62, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 62, 62, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 31, 31, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 31, 31, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 31, 31, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 31, 31, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 31, 31, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 31, 31, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "outer_product (Lambda)          (None, 262144)       0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 20)           5242900     outer_product[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 12,878,164\n",
      "Trainable params: 12,878,164\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "25/24 [==============================] - 20s 790ms/step - loss: 0.4804 - acc: 0.8890 - val_loss: 0.9302 - val_acc: 0.7028\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93024, saving model to Wehigts_final.hdf5\n",
      "Epoch 2/50\n",
      "25/24 [==============================] - 19s 753ms/step - loss: 0.2952 - acc: 0.9458 - val_loss: 0.8691 - val_acc: 0.7054\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93024 to 0.86914, saving model to Wehigts_final.hdf5\n",
      "Epoch 3/50\n",
      "25/24 [==============================] - 19s 757ms/step - loss: 0.2605 - acc: 0.9565 - val_loss: 0.9464 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "25/24 [==============================] - 19s 759ms/step - loss: 0.2508 - acc: 0.9440 - val_loss: 0.8139 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86914 to 0.81394, saving model to Wehigts_final.hdf5\n",
      "Epoch 5/50\n",
      "25/24 [==============================] - 19s 763ms/step - loss: 0.2073 - acc: 0.9662 - val_loss: 0.9291 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "25/24 [==============================] - 20s 781ms/step - loss: 0.1905 - acc: 0.9687 - val_loss: 0.8486 - val_acc: 0.7168\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.1984 - acc: 0.9503 - val_loss: 0.8494 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.1527 - acc: 0.9698 - val_loss: 0.9461 - val_acc: 0.7181\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.1530 - acc: 0.9708 - val_loss: 0.8652 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.1215 - acc: 0.9745 - val_loss: 0.9740 - val_acc: 0.7015\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "25/24 [==============================] - 19s 776ms/step - loss: 0.1399 - acc: 0.9700 - val_loss: 0.8735 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "25/24 [==============================] - 19s 776ms/step - loss: 0.1020 - acc: 0.9750 - val_loss: 0.9253 - val_acc: 0.7245\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0704 - acc: 0.9900 - val_loss: 1.3678 - val_acc: 0.6339\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "25/24 [==============================] - 19s 775ms/step - loss: 0.1070 - acc: 0.9775 - val_loss: 0.8930 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0994 - acc: 0.9803 - val_loss: 1.0338 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0884 - acc: 0.9800 - val_loss: 1.0753 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0722 - acc: 0.9850 - val_loss: 1.0844 - val_acc: 0.7015\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0862 - acc: 0.9800 - val_loss: 1.0449 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.1245 - acc: 0.9675 - val_loss: 0.9194 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "25/24 [==============================] - 20s 787ms/step - loss: 0.0526 - acc: 0.9858 - val_loss: 1.3824 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0517 - acc: 0.9850 - val_loss: 1.1712 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "25/24 [==============================] - 19s 775ms/step - loss: 0.0716 - acc: 0.9862 - val_loss: 1.0186 - val_acc: 0.7283\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0584 - acc: 0.9837 - val_loss: 1.0126 - val_acc: 0.7372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0897 - acc: 0.9740 - val_loss: 0.9107 - val_acc: 0.7564\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      "25/24 [==============================] - 19s 775ms/step - loss: 0.0647 - acc: 0.9858 - val_loss: 0.8978 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "25/24 [==============================] - 19s 775ms/step - loss: 0.0663 - acc: 0.9841 - val_loss: 1.0469 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0709 - acc: 0.9787 - val_loss: 1.9111 - val_acc: 0.5383\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0777 - acc: 0.9798 - val_loss: 1.0004 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0771 - acc: 0.9740 - val_loss: 1.0023 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0308 - acc: 0.9887 - val_loss: 1.1707 - val_acc: 0.7003\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.1019 - acc: 0.9712 - val_loss: 1.0366 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0956 - acc: 0.9700 - val_loss: 0.9953 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0377 - acc: 0.9875 - val_loss: 1.1121 - val_acc: 0.7079\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0525 - acc: 0.9808 - val_loss: 1.1509 - val_acc: 0.7054\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/50\n",
      "25/24 [==============================] - 19s 771ms/step - loss: 0.0364 - acc: 0.9883 - val_loss: 1.0795 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0303 - acc: 0.9920 - val_loss: 1.1430 - val_acc: 0.7181\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0469 - acc: 0.9925 - val_loss: 1.0465 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0750 - acc: 0.9808 - val_loss: 1.1000 - val_acc: 0.7283\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.9981 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0473 - acc: 0.9850 - val_loss: 1.0181 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0500 - acc: 0.9825 - val_loss: 1.0746 - val_acc: 0.7398\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0368 - acc: 0.9887 - val_loss: 1.0489 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.1246 - acc: 0.9708 - val_loss: 2.6419 - val_acc: 0.4247\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0732 - acc: 0.9762 - val_loss: 1.1057 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0543 - acc: 0.9795 - val_loss: 1.8250 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0682 - acc: 0.9740 - val_loss: 1.2599 - val_acc: 0.7207\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0667 - acc: 0.9808 - val_loss: 1.1237 - val_acc: 0.7168\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/50\n",
      "25/24 [==============================] - 19s 773ms/step - loss: 0.0748 - acc: 0.9750 - val_loss: 2.1621 - val_acc: 0.5485\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/50\n",
      "25/24 [==============================] - 19s 772ms/step - loss: 0.0587 - acc: 0.9845 - val_loss: 1.1294 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/50\n",
      "25/24 [==============================] - 19s 774ms/step - loss: 0.0197 - acc: 0.9937 - val_loss: 1.0704 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "#Usar Checpoint mejor de ejecuciÃ³n\n",
    "model.load_weights(checkpoint_path)\n",
    "for j, layer in enumerate(model1.layers):\n",
    "    layer.trainable = True\n",
    "    #if j <14:\n",
    "    #    if \"conv\" in layer.name:\n",
    "    #        layer.trainable = True\n",
    "            \n",
    "#Compile y ejecutar de nuevo\n",
    "#rms = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "#adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "  \n",
    "model.summary()\n",
    "\n",
    "checkpoint_path = \"Wehigts_final.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "## TRAINING with DA and LRA\n",
    "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                            steps_per_epoch=len(x_train) / batch_size, \n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            callbacks=[checkpointer],\n",
    "                            verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
